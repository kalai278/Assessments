{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as psf\nfrom pyspark.sql.functions import dense_rank\n\nsimpleData = [(\"James\", \"Sales\", 3000),\n(\"Michael\", \"Sales\", 4600),\n(\"Robert\", \"Sales\", 4100), \n(\"Maria\", \"Finance\", 3000), \n(\"James\", \"Sales\", 3000),\n(\"Scott\", \"Finance\", 3300), \n(\"Jen\", \"Finance\", 3900),\n(\"Jeff\", \"Marketing\", 3000), \n(\"Kumar\", \"Marketing\", 2000), \n(\"Saif\", \"Sales\", 4100) ]\n \ncolumns= [\"employee_name\", \"department\", \"salary\"] \n\ndf = spark.createDataFrame(data = simpleData, schema = columns) \nwindowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\ndf2=df.withColumn(\"dense_rank\",dense_rank().over(windowSpec))\ndf2.sort(df.department.desc(),df.salary.asc()).show()\n "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38045d95-625a-4077-9752-510afb4045a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n+-------------+----------+------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n+-------------+----------+------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nsimpleData = [(\"i-101\",\"p110\",23, 1),\n(\"i-102\", \"p111\", 50, 1),\n(\"i-103\", \"p111\", 50, 3), \n(\"i-104\", \"p112\", 75, 1), \n(\"i-105\", \"p114\", 125, 1), \n(\"i-106\", \"p115\", 100, 1), \n(\"i-107\", \"p115\", 100, 1), \n(\"i-108\", \"p114\", 125, 2), \n(\"i-109\", \"p113\", 100, 1), \n(\"i-110\", \"p111\", 50, 2)] \n\ncolumns= [\"invoice no\", \"product id\", \"unit_price\", \"quantity\"]\n\ndf = spark.createDataFrame(data = simpleData, schema = columns) \nprint(df.distinct().count())\n\n "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c4c56b9-d7df-4540-b87d-f6e0fd0fb503"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"10\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["10\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"mynotes","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3786195230590692}},"nbformat":4,"nbformat_minor":0}
